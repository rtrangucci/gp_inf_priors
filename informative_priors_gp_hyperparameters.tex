\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage{nips}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{latexsym}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{color}
\usepackage{float}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{MnSymbol}
\usepackage{makeidx}
\usepackage{fancyhdr}
\usepackage{relsize}
\pagestyle{fancy}
\usepackage{lastpage}
\usepackage{url}
\usepackage{mathrsfs}

\newcommand{\F}{\ensuremath{\mathcal F}}
\DeclareMathSymbol{\R}{\mathbin}{AMSb}{"52}
\newcommand{\f}{\ensuremath{\mathcal f}}
\newcommand{\C}{\ensuremath{\mathcal C}}
\newcommand{\M}{\ensuremath{\mathcal M}}
\renewcommand{\H}{\ensuremath{\mathcal H}}
\newcommand{\pisys}{\ensuremath{\mathscr{L}}}
\newcommand{\lsys}[1]{\ensuremath{\lambda \lp #1 \rp}}
\newcommand{\A}{\ensuremath{\mathcal A}}
\newcommand{\E}{\ensuremath{\mathcal E}}
\renewcommand{\L}{\ensuremath{\mathcal L}}
\newcommand{\norm}[1]{\ensuremath{\mathcal \| #1 \|}}
\newcommand{\Exp}[1]{\ensuremath{\mathbb{E} \lb #1 \rb}}
\newcommand{\condExp}[2]{\ensuremath{\mathbb{E} \lb #1 | #2 \rb}}
\newcommand{\lp}{\ensuremath{\left(}}
\newcommand{\rp}{\ensuremath{\right)}}
\newcommand{\lb}{\ensuremath{\left[}}
\newcommand{\rb}{\ensuremath{\right]}}
\newcommand{\B}[1]{\ensuremath{\mathcal B\lp #1 \rp}}
\newcommand{\Pset}[1]{\ensuremath{\mathcal P\lp #1 \rp}}
\newcommand{\siga}[1]{\ensuremath{\sigma\lp #1 \rp}}
\newcommand{\Xrv}[1]{\ensuremath{X\lp #1 \rp}}
\newcommand{\Xrvi}[1]{\ensuremath{X \inv \lp #1 \rp}}
\newcommand{\Yrv}[1]{\ensuremath{Y\lp #1 \rp}}
\newcommand{\Prob}[1]{\ensuremath{\Pb\lp #1 \rp}}
\newcommand{\inv}{\ensuremath{^{-1}}}
\newcommand{\iprod}[2]{\ensuremath{\llangle #1, #2 \rrangle}}
\newcommand{\twopartdef}[4]
{
	\left\{
		\begin{array}{ll}
			#1 & \mbox{if } #2 \\
			#3 & \mbox{if } #4
		\end{array}
	\right.
}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\title{Prior formulation for Gaussian Process Hyperparameters}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Rob Trangucci \\
  Applied Statistics Center\\
  Columbia University\\
  New York, NY 10027 \\
  \texttt{rnt2101@columbia.edu} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Gaussian processes are measures over functions, and as such, can be used as a
  rich prior for functions in Bayesian statistical models. However, the joint
  posterior density of the length scale and marginal variance hyperparameters
  in stationary kernels like the exponentiated quadratic and Matern
  is often weakly informed by the data because of the extreme flexibility of
  Gaussian process priors. We develop a principled approach for specifying
  informative priors for length scale hyperparameters that impose soft
  constraints on the space of functions represented by the Gaussian process
  prior. 
 \end{abstract}

\section{Introduction}

The typical regression setting for observed univariate data
$y_i \in \R, x_i \in \R^M\, i \in {1,\dots,n}$:

\begin{align*}
  \theta & \sim p(\Theta | \phi) \\
  f(x) & \sim GP\lp F(x) |\, \mu(x),
  K_\theta(x, x) \rp \\
  y_i & \sim \mathcal{N}\lp Y | \, f(x), \sigma^2 \rp \forall i \\
\end{align*}

where $GP$ is a stochastic process completely specified its mean vector $\mu$
and its covariance matrix $K_\theta(x, x)$, which is defined as:

\[
  \text{cov}(f(x_i), f(x_j)) = k(x_i, x_j | \theta) 
\]

.   With
$\theta$ fixed, inference of $f(x | \, \mathbf{y}, \theta)$ is straightforward,
%put arrow above y and x to indicate vector of x and y $$:

\begin{align*}
      \begin{pmatrix} f\\ \mathbf{y} \\ \end{pmatrix} & \sim \mathcal{N} \lp
\begin{pmatrix} \mathbf{0} \\ \mathbf{0} \end{pmatrix} ,\,\, \begin{pmatrix}
  K_\theta(\mathbf{x},\mathbf{x}) & K_\theta(\mathbf{x},\mathbf{x})  \\
  K_\theta(\mathbf{x},\mathbf{x}) &  K_\theta(\mathbf{x},\mathbf{x}) + \sigma ^
  2 \mathbf{I}\\ \end{pmatrix} \rp \\ f & \sim
  \mathcal{N}(K_\theta(\mathbf{x},\mathbf{x})(K_\theta(\mathbf{x},\mathbf{x}) +
  \sigma ^ 2 \mathbf{I})^{-1}\mathbf{y},\\ & K_\theta(\mathbf{x},\mathbf{x}) -
  K_\theta(\mathbf{x},\mathbf{x}) (K_\theta(\mathbf{x},\mathbf{x}) + \sigma ^ 2 \mathbf{I})^{-1}K_\theta(\mathbf{x},\mathbf{x}))
\end{align*}

Learning $\theta$ is important to correctly inferring $p(F(x) | y)$. Stein
explores misspecification of Gaussian process kernels and shows that in finite
samples, selecting the wrong point estimate for $\theta$ leads to inflated mean
square error. 

While literature has focused point estimation for $\theta$, we want to allow
for flexibility in our conditional mean function, given that we will select the
wrong conditional mean funciton with probability one for finite data. Thus, we
would like to place priors over $\theta$ integrate over our uncertainty in
$\theta$ when making predictions for new $X$. For maximum flexibility in
model building, we would like to select uninformative priors and run
our MCMC chains to convergence.

Unfortunately, uninformative priors yield large posterior intervals for
hyperparamters and posterior predictive densities using uniformative priors.
This is due to the weak identifiability of the marginal standard deviation
parameter, $\alpha$ and the length scale parameter, $L$, in popular kernels like the
exponentiated quadratic:

\[
  k(x_i, x_j) = \alpha^2 
\exp \left(
	- \dfrac{1}{2L^2} (x_{i} - x_{j})^2
\right)
\]

We can see the weak idenfiability in observable statistics of the process like
the expected number of upcrossings in a unit interval at level $u$, $N_u$. R \&
W write that $\Exp{N_u}$:

\[ 
  \Exp{N_u} = \dfrac{1}{2 \pi} 
  \sqrt{-\dfrac{k^{\prime \prime}(0)}{k(0)}}
  \text{exp}\left(-\dfrac{u^2}{2k(0)}\right)
\]

For the exponentiated quadratic, we see:

\[ 
  \Exp{N_u} = \dfrac{1}{2 \pi L}\text{exp}\left(-\dfrac{u^2}{\alpha ^ 2} \right)
\]

For upcrossings at zero, we're left with 

It also goes unsaid that learning hyperparameters and then learning the
conditional mean function reuses our data twice to infer hyperparamters and
then to infer the conditional mean. Which means that the variance in our
posterior estitmate of the conditional mean function is understated.  Moreover,
and perhaps, most importantly, given our finite sample size, we will not
recover the true hyperparameters in any setting. 

More importantly, R\&W prove that 


Flaxman et. al puts proper priors over $\alpha$, $\sigma$, and $l$, and samples
from the full posterior, integrating over uncertainty in the hyperparameter
estimation, but doesn't explore the properties of different prior formulations.
This is the starting point of our paper. Using the same software as Flaxman, et
al, we explore different prior formulations in the 1-D Gaussian process
regression setting.  This is a deliberately simple model in order to explore
the pathologies of different prior formulations.


Improper learning of $\theta$ can yield poor out-of-sample forecasts 


\end{document}

